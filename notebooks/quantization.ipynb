{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.ao.quantization.quantize_fx as quantize_fx\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as T\n",
    "from models import resnet50\n",
    "from torch.ao.quantization import get_default_qconfig_mapping\n",
    "from torch.utils.data import Subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "model = resnet50(pretrained=True)\n",
    "model.eval()\n",
    "\n",
    "transform = T.Compose([T.ToTensor(), T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
    "\n",
    "full_dataset = datasets.CIFAR10(root=\"./data\", train=False, download=True, transform=transform)\n",
    "subset_indices = np.random.randint(0, len(full_dataset), 1000)\n",
    "subset_dataset = Subset(full_dataset, subset_indices)\n",
    "loader = torch.utils.data.DataLoader(subset_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.966524839401245\n"
     ]
    }
   ],
   "source": [
    "# fp32 での推論\n",
    "answers = []\n",
    "start = time.time()\n",
    "with torch.no_grad():\n",
    "    c = 0\n",
    "    for images, _ in loader:\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        answers.append(predicted.item())\n",
    "print(time.time() - start)\n",
    "# Intel Core i7-12700 で 7.85 秒"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joisino/.cache/pypoetry/virtualenvs/speedbook-5IKDUY1E-py3.11/lib/python3.11/site-packages/torch/ao/quantization/observer.py:220: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GraphModule(\n",
       "  (conv1): QuantizedConvReLU2d(3, 64, kernel_size=(7, 7), stride=(2, 2), scale=0.0030952803790569305, zero_point=0, padding=(3, 3))\n",
       "  (maxpool): Identity()\n",
       "  (layer1): Module(\n",
       "    (0): Module(\n",
       "      (conv1): QuantizedConvReLU2d(64, 64, kernel_size=(1, 1), stride=(1, 1), scale=0.0013514463789761066, zero_point=0)\n",
       "      (conv2): QuantizedConvReLU2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.001981052104383707, zero_point=0, padding=(1, 1))\n",
       "      (conv3): QuantizedConv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), scale=0.0037456878926604986, zero_point=64)\n",
       "      (downsample): Module(\n",
       "        (0): QuantizedConv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), scale=0.004810944199562073, zero_point=73)\n",
       "      )\n",
       "    )\n",
       "    (1): Module(\n",
       "      (conv1): QuantizedConvReLU2d(256, 64, kernel_size=(1, 1), stride=(1, 1), scale=0.001273467089049518, zero_point=0)\n",
       "      (conv2): QuantizedConvReLU2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.0019142132950946689, zero_point=0, padding=(1, 1))\n",
       "      (conv3): QuantizedConv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), scale=0.0021021408028900623, zero_point=67)\n",
       "    )\n",
       "    (2): Module(\n",
       "      (conv1): QuantizedConvReLU2d(256, 64, kernel_size=(1, 1), stride=(1, 1), scale=0.0011841204250231385, zero_point=0)\n",
       "      (conv2): QuantizedConvReLU2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.0015505641931667924, zero_point=0, padding=(1, 1))\n",
       "      (conv3): QuantizedConv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), scale=0.0020206717308610678, zero_point=55)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Module(\n",
       "    (0): Module(\n",
       "      (conv1): QuantizedConvReLU2d(256, 128, kernel_size=(1, 1), stride=(1, 1), scale=0.0014020371017977595, zero_point=0)\n",
       "      (conv2): QuantizedConvReLU2d(128, 128, kernel_size=(3, 3), stride=(2, 2), scale=0.002135302172973752, zero_point=0, padding=(1, 1))\n",
       "      (conv3): QuantizedConv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), scale=0.0022657730150967836, zero_point=55)\n",
       "      (downsample): Module(\n",
       "        (0): QuantizedConv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), scale=0.0022819838486611843, zero_point=57)\n",
       "      )\n",
       "    )\n",
       "    (1): Module(\n",
       "      (conv1): QuantizedConvReLU2d(512, 128, kernel_size=(1, 1), stride=(1, 1), scale=0.0015823872527107596, zero_point=0)\n",
       "      (conv2): QuantizedConvReLU2d(128, 128, kernel_size=(3, 3), stride=(1, 1), scale=0.0012347821611911058, zero_point=0, padding=(1, 1))\n",
       "      (conv3): QuantizedConv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), scale=0.0015052913222461939, zero_point=68)\n",
       "    )\n",
       "    (2): Module(\n",
       "      (conv1): QuantizedConvReLU2d(512, 128, kernel_size=(1, 1), stride=(1, 1), scale=0.0016674355138093233, zero_point=0)\n",
       "      (conv2): QuantizedConvReLU2d(128, 128, kernel_size=(3, 3), stride=(1, 1), scale=0.00178897671867162, zero_point=0, padding=(1, 1))\n",
       "      (conv3): QuantizedConv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), scale=0.0020493706688284874, zero_point=77)\n",
       "    )\n",
       "    (3): Module(\n",
       "      (conv1): QuantizedConvReLU2d(512, 128, kernel_size=(1, 1), stride=(1, 1), scale=0.001467213500291109, zero_point=0)\n",
       "      (conv2): QuantizedConvReLU2d(128, 128, kernel_size=(3, 3), stride=(1, 1), scale=0.00186191126704216, zero_point=0, padding=(1, 1))\n",
       "      (conv3): QuantizedConv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), scale=0.0018469119677320123, zero_point=61)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Module(\n",
       "    (0): Module(\n",
       "      (conv1): QuantizedConvReLU2d(512, 256, kernel_size=(1, 1), stride=(1, 1), scale=0.0012022904120385647, zero_point=0)\n",
       "      (conv2): QuantizedConvReLU2d(256, 256, kernel_size=(3, 3), stride=(2, 2), scale=0.001879435614682734, zero_point=0, padding=(1, 1))\n",
       "      (conv3): QuantizedConv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), scale=0.0014810977736487985, zero_point=54)\n",
       "      (downsample): Module(\n",
       "        (0): QuantizedConv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), scale=0.0007655388326384127, zero_point=60)\n",
       "      )\n",
       "    )\n",
       "    (1): Module(\n",
       "      (conv1): QuantizedConvReLU2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), scale=0.0008988735498860478, zero_point=0)\n",
       "      (conv2): QuantizedConvReLU2d(256, 256, kernel_size=(3, 3), stride=(1, 1), scale=0.0010082836961373687, zero_point=0, padding=(1, 1))\n",
       "      (conv3): QuantizedConv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), scale=0.0015614579897373915, zero_point=60)\n",
       "    )\n",
       "    (2): Module(\n",
       "      (conv1): QuantizedConvReLU2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), scale=0.0009834147058427334, zero_point=0)\n",
       "      (conv2): QuantizedConvReLU2d(256, 256, kernel_size=(3, 3), stride=(1, 1), scale=0.0011744450312107801, zero_point=0, padding=(1, 1))\n",
       "      (conv3): QuantizedConv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), scale=0.0012570066610351205, zero_point=53)\n",
       "    )\n",
       "    (3): Module(\n",
       "      (conv1): QuantizedConvReLU2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), scale=0.0010817466536536813, zero_point=0)\n",
       "      (conv2): QuantizedConvReLU2d(256, 256, kernel_size=(3, 3), stride=(1, 1), scale=0.000852591241709888, zero_point=0, padding=(1, 1))\n",
       "      (conv3): QuantizedConv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), scale=0.0014876272762194276, zero_point=46)\n",
       "    )\n",
       "    (4): Module(\n",
       "      (conv1): QuantizedConvReLU2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), scale=0.0008722660131752491, zero_point=0)\n",
       "      (conv2): QuantizedConvReLU2d(256, 256, kernel_size=(3, 3), stride=(1, 1), scale=0.001413685386069119, zero_point=0, padding=(1, 1))\n",
       "      (conv3): QuantizedConv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), scale=0.0026445917319506407, zero_point=58)\n",
       "    )\n",
       "    (5): Module(\n",
       "      (conv1): QuantizedConvReLU2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), scale=0.0012566926889121532, zero_point=0)\n",
       "      (conv2): QuantizedConvReLU2d(256, 256, kernel_size=(3, 3), stride=(1, 1), scale=0.0013542681699618697, zero_point=0, padding=(1, 1))\n",
       "      (conv3): QuantizedConv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), scale=0.004758057184517384, zero_point=30)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Module(\n",
       "    (0): Module(\n",
       "      (conv1): QuantizedConvReLU2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), scale=0.0007540247752331197, zero_point=0)\n",
       "      (conv2): QuantizedConvReLU2d(512, 512, kernel_size=(3, 3), stride=(2, 2), scale=0.0021139278542250395, zero_point=0, padding=(1, 1))\n",
       "      (conv3): QuantizedConv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), scale=0.002861532848328352, zero_point=8)\n",
       "      (downsample): Module(\n",
       "        (0): QuantizedConv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), scale=0.0028235609643161297, zero_point=14)\n",
       "      )\n",
       "    )\n",
       "    (1): Module(\n",
       "      (conv1): QuantizedConvReLU2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), scale=0.0012569550890475512, zero_point=0)\n",
       "      (conv2): QuantizedConvReLU2d(512, 512, kernel_size=(3, 3), stride=(1, 1), scale=0.0029884050600230694, zero_point=0, padding=(1, 1))\n",
       "      (conv3): QuantizedConv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), scale=0.0027343847323209047, zero_point=5)\n",
       "    )\n",
       "    (2): Module(\n",
       "      (conv1): QuantizedConvReLU2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), scale=0.0012223799712955952, zero_point=0)\n",
       "      (conv2): QuantizedConvReLU2d(512, 512, kernel_size=(3, 3), stride=(1, 1), scale=0.0027993610128760338, zero_point=0, padding=(1, 1))\n",
       "      (conv3): QuantizedConv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), scale=0.0026513373013585806, zero_point=5)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): QuantizedLinear(in_features=2048, out_features=10, scale=0.06282773613929749, zero_point=22, qscheme=torch.per_channel_affine)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 準備\n",
    "qconfig_mapping = get_default_qconfig_mapping(\"x86\")\n",
    "model_prepared = quantize_fx.prepare_fx(model, qconfig_mapping, full_dataset.data[0])\n",
    "\n",
    "# 較正\n",
    "with torch.no_grad():\n",
    "    for images, _ in loader:\n",
    "        model_prepared(images)\n",
    "\n",
    "# 量子化\n",
    "model_quantized = quantize_fx.convert_fx(model_prepared)\n",
    "model_quantized.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.5624163150787354\n"
     ]
    }
   ],
   "source": [
    "# INT8 での推論\n",
    "answers_quantized = []\n",
    "start = time.time()\n",
    "with torch.no_grad():\n",
    "    for images, _ in loader:\n",
    "        outputs = model_quantized(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        answers_quantized.append(predicted.item())\n",
    "print(time.time() - start)\n",
    "# Intel Core i7-12700 で 4.41 秒"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.994\n"
     ]
    }
   ],
   "source": [
    "# 一致度\n",
    "print(\"acc:\", sum([answers[i] == answers_quantized[i] for i in range(len(answers))]) / len(answers))\n",
    "# -> 0.994"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
